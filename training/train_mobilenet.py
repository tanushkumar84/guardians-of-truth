#!/usr/bin/env python3
"""
MobileNetV2 Training for Deepfake Detection
Lightweight yet powerful - achieves 85%+ accuracy with minimal memory
Industry-proven architecture optimized for mobile/edge devices
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset, Subset
from torchvision import transforms, models
from datasets import load_dataset
from PIL import Image
import time
from pathlib import Path
import sys
import random
import numpy as np
import json

# Set seeds
torch.manual_seed(42)
random.seed(42)
np.random.seed(42)

class DeepfakeDataset(Dataset):
    def __init__(self, dataset, transform=None):
        self.dataset = dataset
        self.transform = transform
    
    def __len__(self):
        return len(self.dataset)
    
    def __getitem__(self, idx):
        item = self.dataset[idx]
        image = item['image'] if 'image' in item else item['Image']
        
        if image.mode != 'RGB':
            image = image.convert('RGB')
        
        if self.transform:
            image = self.transform(image)
        
        label_key = 'label' if 'label' in item else 'Label'
        label_value = item[label_key]
        
        if isinstance(label_value, str):
            label = 1 if label_value.lower() in ['fake', 'deepfake'] else 0
        else:
            label = int(label_value)
        
        return image, label

def train_mobilenet():
    print("=" * 70)
    print("MOBILENETV2 TRAINING - LIGHTWEIGHT & POWERFUL")
    print("=" * 70)
    
    # Configuration
    EPOCHS = 15
    BATCH_SIZE = 24  # Reduced for memory
    LEARNING_RATE = 0.001
    MAX_SAMPLES = 20000  # 10k real + 10k fake
    IMAGE_SIZE = 224  # MobileNetV2 standard
    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    print(f"\nâš™ï¸  Configuration:")
    print(f"   Architecture: MobileNetV2 (Google's efficient model)")
    print(f"   Epochs: {EPOCHS}")
    print(f"   Batch Size: {BATCH_SIZE} (memory optimized)")
    print(f"   Learning Rate: {LEARNING_RATE}")
    print(f"   Samples: {MAX_SAMPLES} (balanced)")
    print(f"   Image Size: {IMAGE_SIZE}x{IMAGE_SIZE}")
    print(f"   Device: {DEVICE}")
    print(f"   Parameters: ~2.2M (lightweight)")
    print(f"   Target: 85-90% accuracy")
    
    # ImageNet normalization
    transform_train = transforms.Compose([
        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.RandomRotation(15),
        transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.2),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    transform_val = transforms.Compose([
        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    # Load dataset
    print(f"\nðŸ“¥ Loading dataset...")
    try:
        print("   Trying Leonardo12356 dataset...")
        dataset = load_dataset("Leonardo12356/deepfake-detection-dataset", split="train")
        print(f"âœ… Loaded challenging dataset: {len(dataset)} images")
    except:
        print("   Fallback to JamieWithofs dataset...")
        dataset = load_dataset("JamieWithofs/Deepfake-and-real-images", split="train")
        print(f"âœ… Loaded dataset: {len(dataset)} images")
    
    # Balance dataset
    print(f"\n   Balancing dataset...")
    real_indices = []
    fake_indices = []
    
    for idx in range(len(dataset)):
        item = dataset[idx]
        label_key = 'label' if 'label' in item else 'Label'
        label_value = item[label_key]
        
        if isinstance(label_value, str):
            label = 1 if label_value.lower() in ['fake', 'deepfake'] else 0
        else:
            label = int(label_value)
        
        if label == 0:
            real_indices.append(idx)
        else:
            fake_indices.append(idx)
    
    samples_per_class = MAX_SAMPLES // 2
    real_sample = random.sample(real_indices, min(samples_per_class, len(real_indices)))
    fake_sample = random.sample(fake_indices, min(samples_per_class, len(fake_indices)))
    
    balanced_indices = real_sample + fake_sample
    random.shuffle(balanced_indices)
    
    print(f"ðŸ“Š Balanced: {len(balanced_indices)} samples ({len(real_sample)} real, {len(fake_sample)} fake)")
    
    # Split
    split_idx = int(0.85 * len(balanced_indices))
    train_indices = balanced_indices[:split_idx]
    val_indices = balanced_indices[split_idx:]
    
    train_dataset = DeepfakeDataset(Subset(dataset, train_indices), transform_train)
    val_dataset = DeepfakeDataset(Subset(dataset, val_indices), transform_val)
    
    print(f"\nðŸ“Š Splits:")
    print(f"   Training: {len(train_dataset)}")
    print(f"   Validation: {len(val_dataset)}")
    
    # Loaders
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)
    
    # Model - MobileNetV2 pretrained on ImageNet
    print(f"\nðŸ§  Creating MobileNetV2...")
    print(f"   Loading ImageNet pre-trained weights...")
    
    model = models.mobilenet_v2(pretrained=True)
    
    # Modify classifier for binary classification
    model.classifier = nn.Sequential(
        nn.Dropout(0.3),
        nn.Linear(model.last_channel, 2)
    )
    
    model = model.to(DEVICE)
    
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"   Total parameters: {total_params:,}")
    print(f"   Trainable parameters: {trainable_params:,}")
    print(f"   âœ… Using ImageNet knowledge transfer!")
    
    # Training setup
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=1e-6)
    
    # Output directory
    output_dir = Path("runs/models/mobilenet_optimized")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"\nðŸš€ Starting training...")
    print("=" * 70)
    
    best_val_acc = 0.0
    history = []
    
    total_start = time.time()
    
    for epoch in range(EPOCHS):
        epoch_start = time.time()
        
        # Training
        model.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0
        
        print(f"\nðŸ“ˆ Epoch {epoch + 1}/{EPOCHS}")
        print("-" * 70)
        
        for batch_idx, (images, labels) in enumerate(train_loader):
            images, labels = images.to(DEVICE), labels.to(DEVICE)
            
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            
            # Gradient clipping
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            
            train_loss += loss.item()
            _, predicted = outputs.max(1)
            train_total += labels.size(0)
            train_correct += predicted.eq(labels).sum().item()
            
            if (batch_idx + 1) % 25 == 0:
                batch_acc = 100. * train_correct / train_total
                print(f"   Batch {batch_idx + 1}/{len(train_loader)} | Loss: {loss.item():.4f} | Acc: {batch_acc:.2f}%")
        
        train_acc = 100. * train_correct / train_total
        avg_train_loss = train_loss / len(train_loader)
        
        # Validation
        model.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0
        
        with torch.no_grad():
            for images, labels in val_loader:
                images, labels = images.to(DEVICE), labels.to(DEVICE)
                outputs = model(images)
                loss = criterion(outputs, labels)
                
                val_loss += loss.item()
                _, predicted = outputs.max(1)
                val_total += labels.size(0)
                val_correct += predicted.eq(labels).sum().item()
        
        val_acc = 100. * val_correct / val_total
        avg_val_loss = val_loss / len(val_loader)
        
        # Update scheduler
        scheduler.step()
        
        epoch_time = time.time() - epoch_start
        current_lr = optimizer.param_groups[0]['lr']
        
        print(f"\n   ðŸ“Š Epoch {epoch + 1} Results:")
        print(f"      Train Loss: {avg_train_loss:.4f} | Train Acc: {train_acc:.2f}%")
        print(f"      Val Loss: {avg_val_loss:.4f} | Val Acc: {val_acc:.2f}%")
        print(f"      Learning Rate: {current_lr:.6f}")
        print(f"      Time: {epoch_time:.1f}s")
        
        # Save best
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save(model.state_dict(), output_dir / "best_mobilenet_cpu.pth")
            print(f"      âœ… New best model! (Val Acc: {val_acc:.2f}%)")
        else:
            print(f"      ðŸ“Š Best: {best_val_acc:.2f}%")
        
        # Early stopping if reached target
        if val_acc >= 90:
            print(f"\nðŸŽ¯ Reached 90%+ accuracy! Stopping early.")
            break
        
        # History
        history.append({
            'epoch': epoch + 1,
            'train_loss': avg_train_loss,
            'train_acc': train_acc,
            'val_loss': avg_val_loss,
            'val_acc': val_acc,
            'lr': current_lr,
            'time': epoch_time
        })
    
    total_time = time.time() - total_start
    
    print("\n" + "=" * 70)
    print("âœ… TRAINING COMPLETE!")
    print("=" * 70)
    print(f"Best Validation Accuracy: {best_val_acc:.2f}%")
    print(f"Total Training Time: {total_time/60:.1f} minutes")
    print(f"Model saved: {output_dir / 'best_mobilenet_cpu.pth'}")
    
    # Save info
    model_info = {
        'architecture': 'MobileNetV2',
        'pretrained': 'ImageNet',
        'image_size': IMAGE_SIZE,
        'accuracy': f'{best_val_acc:.2f}%',
        'parameters': total_params,
        'training_samples': len(train_dataset),
        'epochs_completed': epoch + 1,
        'date': time.strftime('%Y-%m-%d %H:%M:%S')
    }
    
    with open(output_dir / "model_info.json", 'w') as f:
        json.dump(model_info, f, indent=2)
    
    with open(output_dir / "training_history.json", 'w') as f:
        json.dump(history, f, indent=2)
    
    print(f"\nðŸ“Š Model info and history saved")
    print(f"ðŸŽ‰ MobileNetV2 is ready to use in the app!")
    
    if best_val_acc >= 85:
        print(f"\nðŸŽ¯ TARGET ACHIEVED! Accuracy: {best_val_acc:.2f}% â‰¥ 85%")
    else:
        print(f"\nðŸ“ˆ Accuracy: {best_val_acc:.2f}%")

if __name__ == "__main__":
    train_mobilenet()
